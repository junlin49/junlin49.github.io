<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>OpenCV中的Aruco识别 | Go Further</title><meta name="description" content="OpenCV中的Aruco识别 转载博客： https:&#x2F;&#x2F;blog.csdn.net&#x2F;u010260681&#x2F;article&#x2F;details&#x2F;77089657 如有侵权，立即删除  姿态估计（Pose estimation）在计算机视觉领域扮演着十分重要的角色：机器人导航、增强现实以及其它。这一过程的基础是找到现实世界和图像投影之间的对应点。这通常是很困难的一步，因此我们常常用自己制作的或基本的M"><meta name="keywords" content="Aruco"><meta name="author" content="Wang"><meta name="copyright" content="Wang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://junlu.club/2020/09/17/OpenCV%E4%B8%AD%E7%9A%84Aruco%E8%AF%86%E5%88%AB/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="OpenCV中的Aruco识别"><meta property="og:url" content="http://junlu.club/2020/09/17/OpenCV%E4%B8%AD%E7%9A%84Aruco%E8%AF%86%E5%88%AB/"><meta property="og:site_name" content="Go Further"><meta property="og:description" content="OpenCV中的Aruco识别 转载博客： https:&#x2F;&#x2F;blog.csdn.net&#x2F;u010260681&#x2F;article&#x2F;details&#x2F;77089657 如有侵权，立即删除  姿态估计（Pose estimation）在计算机视觉领域扮演着十分重要的角色：机器人导航、增强现实以及其它。这一过程的基础是找到现实世界和图像投影之间的对应点。这通常是很困难的一步，因此我们常常用自己制作的或基本的M"><meta property="og:image" content="http://junlu.club/img/av.jpg"><meta property="article:published_time" content="2020-09-17T02:21:35.000Z"><meta property="article:modified_time" content="2020-09-21T16:11:32.000Z"><meta name="twitter:card" content="summary"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.3.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime: '',
  date_suffix: {"one_hour":"刚刚","hours":"小时前","day":"天前"},
  copyright: {"limitCount":500,"languages":{"author":"作者: Wang","link":"链接: ","source":"来源: Go Further","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: {"text":"爱国,敬业,诚信,友善,和谐,富强","fontSize":"15px"},
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
    },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isSidebar: true,
  postUpdate: '2020-09-22 00:11:32'
}</script><noscript><style type="text/css">
#nav {
  opacity: 1
}
.justified-gallery img {
  opacity: 1
}
</style></noscript><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
  }
}

var autoChangeMode = 'false'
var t = saveToLocal.get('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (saveToLocal.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><meta name="referrer" content="no-referrer-when-downgrade" /><meta name="generator" content="Hexo 5.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/av.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">69</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">45</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#OpenCV%E4%B8%AD%E7%9A%84Aruco%E8%AF%86%E5%88%AB"><span class="toc-number">1.</span> <span class="toc-text">OpenCV中的Aruco识别</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Marker%E5%92%8C%E5%AD%97%E5%85%B8"><span class="toc-number">1.1.</span> <span class="toc-text">Marker和字典</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAMarker"><span class="toc-number">1.2.</span> <span class="toc-text">创建Marker</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E6%B5%8BMarker"><span class="toc-number">1.3.</span> <span class="toc-text">检测Marker</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pose%E6%A3%80%E6%B5%8B"><span class="toc-number">1.4.</span> <span class="toc-text">Pose检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E5%AD%97%E5%85%B8"><span class="toc-number">1.5.</span> <span class="toc-text">选择字典</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E6%B5%8B%E5%8F%82%E6%95%B0"><span class="toc-number">1.6.</span> <span class="toc-text">检测参数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%98%88%E5%80%BC%E5%8C%96"><span class="toc-number">1.6.0.1.</span> <span class="toc-text">阈值化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AE%E5%BB%93%E6%BB%A4%E6%B3%A2"><span class="toc-number">1.6.0.2.</span> <span class="toc-text">轮廓滤波</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AF%94%E7%89%B9%E4%BD%8D%E6%8F%90%E5%8F%96"><span class="toc-number">1.6.0.3.</span> <span class="toc-text">比特位提取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Marker-ID"><span class="toc-number">1.6.0.4.</span> <span class="toc-text">Marker ID</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%92%E8%90%BD%E7%BB%86%E5%8C%96%EF%BC%88Corner-Refinement%EF%BC%89"><span class="toc-number">1.6.0.5.</span> <span class="toc-text">角落细化（Corner Refinement）</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Go Further</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">OpenCV中的Aruco识别</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-09-17T02:21:35.000Z" title="发表于 2020-09-17 10:21:35">2020-09-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-09-21T16:11:32.000Z" title="更新于 2020-09-22 00:11:32">2020-09-22</time></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>21分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="OpenCV中的Aruco识别"><a href="#OpenCV中的Aruco识别" class="headerlink" title="OpenCV中的Aruco识别"></a>OpenCV中的Aruco识别</h1><blockquote>
<p>转载博客：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010260681/article/details/77089657">https://blog.csdn.net/u010260681/article/details/77089657</a></p>
<p>如有侵权，立即删除</p>
</blockquote>
<p>姿态估计（Pose estimation）在<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/computervison">计算机视觉</a>领域扮演着十分重要的角色：<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/robot">机器人</a>导航、增强现实以及其它。这一过程的基础是找到现实世界和图像投影之间的对应点。这通常是很困难的一步，因此我们常常用自己制作的或基本的Marker来让这一切变得更容易。</p>
<p>最为流行的一个途径是基于二进制平方的标记。这种Marker的主要便利之处在于，一个Marker提供了足够多的对应（四个角）来获取相机的信息。同样的，内部的二进制编码使得<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/datastructure">算法</a>非常健壮，允许应用错误检测和校正技术的可能性。</p>
<p>aruco模块基于ArUco库，这是一个检测二进制marker的非常流行的库，是由Rafael Muñoz和Sergio Garrido完成的。</p>
<p>aruco的函数包含在c++：<a target="_blank" rel="noopener" href="http://docs.opencv.org/3.1.0/d9/d53/aruco_8hpp.html">链接地址</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;opencv2&#x2F;aruco.hpp&gt;</span><br></pre></td></tr></table></figure>
<h2 id="Marker和字典"><a href="#Marker和字典" class="headerlink" title="Marker和字典"></a>Marker和字典</h2><p>一个ArUco marker是一个二进制平方标记，它由一个宽的黑边和一个内部的二进制矩阵组成，内部的矩阵决定了它们的id。黑色的边界有利于快速检测到图像，二进制编码可以验证id，并且允许错误检测和矫正技术的应用。marker的大小决定了内部矩阵的大小。例如，一个4x4的marker由16bits组成。</p>
<p>一些ArUco markers的例子：</p>
<p><img src="http://docs.opencv.org/3.1.0/markers.jpg" alt="markers.jpg"></p>
<p>应当注意到，我们需要检测到一个Marker在空间中发生了旋转，但是，检测的过程需要确定它的初始角度，所以每个角落需要是明确的，不能有歧义，保证上述这点也是靠二进制编码完成的。</p>
<p>markers的字典是在一个特殊应用中使用到的marker的集合。这仅仅是每个marker的二进制编码的链表。</p>
<p>字典的主要性质是字典的大小和marker的大小：</p>
<ul>
<li>字典的大小是组成字典的marker的数量</li>
<li><p>marker的大小是这些marker的尺寸（位的个数）</p>
<p>aruco模块包含了一些预定义的字典，这些字典涵盖了一系列的字典大小和Marker尺寸。</p>
</li>
</ul>
<p>有些人可能会认为Marker的id是从十进制转成二进制的。但是，考虑到较大的marker会有较多的位数，管理如此多的数据不那么现实，这并不可能。反之，一个marker的id仅需是marker在它所在的字典的下标。例如，一个字典里的五个marker的id是：0,1,2,3和4。</p>
<p>更多有关字典的信息在“选择字典”部分提及。</p>
<h2 id="创建Marker"><a href="#创建Marker" class="headerlink" title="创建Marker"></a>创建Marker</h2><p>​    在检测之前，我们需要打印marker，以把它们放到环境中。marker的图像可以使用<a target="_blank" rel="noopener" href="http://docs.opencv.org/3.1.0/d6/d6e/group__imgproc__draw.html#ga482fa7b0f578fcdd8a174904592a6250">drawMarker()</a>函数生成。</p>
<p>​    例如，让我们分析一下如下的调用：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cv::Mat markerImage;</span><br><span class="line">cv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250);</span><br><span class="line"></span><br><span class="line">  cv::aruco::drawMarker(dictionary, <span class="number">23</span>, <span class="number">200</span>, markerImage, <span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>首先，我们通过选择aruco模块中一个预定义的字典来创建一个字典对象，具体而言，这个字典是由250个marker组成的，每个marker的大小为6x6bits(DICT_6X6_250)</p>
<p>​     <code>drawMarker</code>的参数如下：</p>
<ul>
<li>第一个参数是之前创建的字典对象。</li>
<li>第二个参数是marker的id，在这个例子中选择的是字典DICT_6X6_250第23个marker。注意到每个字典是由不同数目的Marker组成的，在这个例子中，有效的Id数字范围是0到249。不在有效区间的特定id将会产生异常。</li>
<li>第三个参数，200，是输出Marker图像的大小。在这个例子中，输出的图像将是200x200像素大小。注意到这一参数需要满足能够存储特定字典 的所有位。所以，举例而言，你不能为6x6大小的marker生成一个5x5图像（这还没有考虑到Marker的边界）。除此之外，为了避免变形，这一参数最好和位数+边界的大小成正比，至少要比marker的大小大得多（如这个例子中的200)，这样变形就不显著了。</li>
<li>第四个参数是输出的图像。</li>
<li>最终，最后一个参数是一个可选的参数，它指定了Marer黑色边界的大小。这一大小与位数数目成正比。例如，值为2意味着边界的宽度将会是2的倍数。默认的值为1。</li>
<li>生成的图像如下：</li>
</ul>
<p><img src="http://docs.opencv.org/3.1.0/marker23.jpg" alt="marker23.jpg"></p>
<p>​    详细的例子在模块演示文件夹中的 <code>create_marker.cpp</code> </p>
<h2 id="检测Marker"><a href="#检测Marker" class="headerlink" title="检测Marker"></a>检测Marker</h2><p>给定一个可以看见ArUco marker的图像，检测程序应当返回检测到的marker的列表。每个检测到的marker包括：</p>
<ul>
<li>图像四个角的位置（按照原始的顺序）</li>
<li>marker的Id</li>
</ul>
<p>marker检测过程由以下两个主要步骤构成：</p>
<ol>
<li>检测有哪些marker。在这一阶段我们分析图像，以找到哪些形状可以被识别为Markers。首先要做的是利用自适应性阈值来分割marker，然后从阈值化的图像中提取外形轮廓，并且舍弃那些非凸多边形的，以及那些不是方形的。我们还使用了一些额外的滤波（来剔除那些过小或者过大的轮廓，过于相近的凸多边形，等）</li>
<li>检测完marker之后，我们有必要分析它的内部编码来确定它们是否确实是marker。此步骤首先提取每个标记的标记位。为了达到这个目的，首先，我们需要对图像进行透视变换，来得到它规范的形态（正视图）。然后，对规范的图像用Ossu阈值化以分离白色和黑色位。这一图像根据marker大小和边界大小被分为不同格子，我们统计落在每个格子中的黑白像素数目来决定这是黑色还是白色的位。最终，我们分析这些位数来决定这个marker是属于哪个特定字典的，如果有必要的话，需要对错误进行检测。</li>
</ol>
<p>考虑如下图像：</p>
<p><img src="http://docs.opencv.org/3.1.0/singlemarkersoriginal.png" alt="singlemarkersoriginal.png"></p>
<p>这些是检测出来的marker（用绿色标记）</p>
<p><img src="http://docs.opencv.org/3.1.0/singlemarkersdetection.png" alt="singlemarkersdetection.png"></p>
<p>以下是识别阶段被剔除的Marker候选（用粉红色标记）：</p>
<p><img src="http://docs.opencv.org/3.1.0/singlemarkersrejected.png" alt="singlemarkersrejected.png"></p>
<p>在aruco模块，检测是由<code>detectMarkers()</code> 函数完成的，这一函数是这个模块中最重要的函数，因为剩下的所有函数操作都基于detectMarkers()返回的检测出的markers。</p>
<p>一个marker检测的例子：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cv::Mat inputImage;</span><br><span class="line"><span class="built_in">vector</span>&lt; <span class="keyword">int</span> &gt; markerIds; </span><br><span class="line"><span class="built_in">vector</span>&lt; <span class="built_in">vector</span>&lt;Point2f&gt; &gt; markerCorners, rejectedCandidates; </span><br><span class="line">cv::aruco::DetectorParameters parameters; </span><br><span class="line">cv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250); </span><br><span class="line">cv::aruco::detectMarkers(inputImage, dictionary, markerCorners, markerIds, parameters, rejectedCandidates); </span><br></pre></td></tr></table></figure>
<p><code>detectMarkers</code> 的参数为：</p>
<ul>
<li>第一个参数是待检测marker的图像。</li>
<li>第二个参数是字典对象，在这一例子中是之前定义的字典 (<code>DICT_6X6_250</code>).</li>
<li>检测出的markers存储在<code>markerCorners</code> 和 <code>markerIds</code>结构中：<ul>
<li><code>markerCorners</code> 是检测出的图像的角的列表。对于每个marker，将返回按照原始顺序排列的四个角（从左上角顺时针开始）。因此，第一个点是左上角的角，紧接着右上角、右下角和左下角。</li>
<li><code>markerIds</code> 是在markerCorners检测出的所有maker的id列表.注意返回的markerCorners和<code>markerIds</code> 向量具有相同的大小。 </li>
</ul>
</li>
<li>第四个参数是类型的对象 <code>DetectionParameters</code>. 这一对象包含了检测阶段的所有参数。这一参数将在 下一章节详细介绍。</li>
<li><p>最后的参数, <code>rejectedCandidates</code>, 返回了所有的marker候选, 例如， 那些被检测出来的不是有效编码的方形。每个候选同样由四个角定义， 它的 形式和markerCorners的参数一样。这一参数可以省略，它仅仅用于debug阶段，或是用于“再次寻找”策略（见<a target="_blank" rel="noopener" href="http://docs.opencv.org/3.1.0/d9/d6a/group__aruco.html#ga90374a799f1da566e5de16f277b12463">refineDetectedMarkers()</a>）</p>
<p><code>detectMarkers()</code>之后，接下来你想要做的事情可能是检查你的marker是否被正确地检测出来了。幸运的是，aruco模块提供了一个函数，它能在输入图像中来绘制检测出来的markers，这个函数就是<a target="_blank" rel="noopener" href="http://docs.opencv.org/3.1.0/d9/d6a/group__aruco.html#ga2ad34b0f277edebb6a132d3069ed2909">drawDetectedMarkers()</a> ,例子如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cv::Mat outputImage;</span><br><span class="line">cv::aruco::drawDetectedMarkers(image, markerCorners, markerIds);</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>image</code> 是输入/输出图像，程序将在这张图上绘制marker。（它通常就是检测出marker的那张图像）</p>
</li>
<li><code>markerCorners</code> 和 <code>markerIds</code> 是检测出marker的结构，它们的格式和 <code>detectMarkers()</code>函数提供的一样。</li>
</ul>
<p><img src="http://docs.opencv.org/3.1.0/singlemarkersdetection.png" alt="singlemarkersdetection.png"></p>
<p> 注意到这个函数仅仅用于可视化，而没有别的什么用途。</p>
<p>使用这两个函数我们完成了基本的marker识别步骤，我们可以从相机中检测出Marker了。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cv::VideoCapture inputVideo; </span><br><span class="line"></span><br><span class="line">inputVideo.open(<span class="number">0</span>); </span><br><span class="line"></span><br><span class="line">cv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250); </span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (inputVideo.grab()) </span><br><span class="line"></span><br><span class="line">&#123; </span><br><span class="line">    cv::Mat image, imageCopy;</span><br><span class="line">    inputVideo.retrieve(image); </span><br><span class="line">    image.copyTo(imageCopy);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ids; <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;cv::Point2f&gt; &gt; corners; </span><br><span class="line">    cv::aruco::detectMarkers(image, dictionary, corners, ids);</span><br><span class="line">    <span class="comment">// if at least one marker detected </span></span><br><span class="line">    <span class="keyword">if</span> (ids.size() &gt; <span class="number">0</span>) </span><br><span class="line">        cv::aruco::drawDetectedMarkers(imageCopy, corners, ids);</span><br><span class="line">    cv::imshow(<span class="string">&quot;out&quot;</span>, imageCopy); </span><br><span class="line">    <span class="keyword">char</span> key = (<span class="keyword">char</span>) cv::waitKey(waitTime); </span><br><span class="line">    <span class="keyword">if</span> (key == <span class="number">27</span>) <span class="keyword">break</span>; </span><br><span class="line"></span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>​    注意到这里忽略了有些可选的参数，比如检测参数对象、以及输出的被剔除的候选对象向量。</p>
<p>​    完整的工程代码包含在模块样例文件夹中的<code>detect_markers.cpp</code> </p>
<h2 id="Pose检测"><a href="#Pose检测" class="headerlink" title="Pose检测"></a>Pose检测</h2><p>接下来你想要做的应当是通过Marker检测来获取相机pose。</p>
<p>为了展现相机的Pose检测，你需要知道你的相机的校准（Calibration）参数。这是一个相机矩阵和畸变系数。如果你不知道如何校准你的相机，你可以看一看<code>calibrateCamera()</code> 函数，以及<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/opencv">OpenCV</a>的校准教程。你同样可以使用aruco模块来校准你的相机，这在使用aruco进行校准的教程中将会介绍。注意这个过程只需要做一次，除非你的相机的光学性质发生了改变（例如调焦）</p>
<p>最后，在校准之后我们得到的是相机矩阵：这是一个3x3的矩阵，包含了焦距和相机中心坐标（相机的内参），以及畸变系数：一个包含五个以上元素的向量，它描述的是相机产生的畸变。</p>
<p>当你用ArUco marker来检测相机Pose时，你可以单独地检测每个Marker的pose。如果你想要从一堆Marker里检测出一个pose，你需要的是aruco板。（参见ArUco板教程）</p>
<p>涉及到marker的相机pose是一个从marker坐标系统到相机坐标系统的三维变换。这是由一个旋转和一个平移向量确定的（参见 <code>solvePnP()</code> 函数）</p>
<p>aruco模块提供了一个函数，用来检测所有探测到的Marker的pose。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">Mat cameraMatrix, distCoeffs; </span><br><span class="line"></span><br><span class="line">... </span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt; Vec3d &gt; rvecs, tvecs; </span><br><span class="line"></span><br><span class="line">cv::aruco::estimatePoseSingleMarkers(corners, <span class="number">0.05</span>, cameraMatrix, distCoeffs, rvecs, tvecs); </span><br></pre></td></tr></table></figure>
<ul>
<li><code>corners</code> 参数是marker的角向量，是由<code>detectMarkers()</code> 函数返回的。</li>
<li>第二个参数是marker的大小（单位是米或者其它）。注意Pose检测的平移矩阵单位都是相同的。</li>
<li><code>cameraMatrix</code> 和 <code>distCoeffs</code> 是需要求解的相机校准参数。</li>
<li><p><code>rvecs</code> 和 <code>tvecs</code> 分别是每个markers角的旋转和平移向量。</p>
<p>这一函数获取的marker坐标系统处在marker重心，Z坐标指向纸面外部，如下图所示。坐标的颜色为,X:红色，Y：绿色，Z：蓝色。</p>
</li>
</ul>
<p><img src="http://docs.opencv.org/3.1.0/singlemarkersaxis.png" alt="singlemarkersaxis.png"></p>
<p>aruco模块提供了一个函数绘制上图中的坐标，所以我们可以检查pose检测的正确性。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cv::aruco::drawAxis (image, cameraMatrix, distCoeffs, rvec, tvec, <span class="number">0.1</span>); </span><br></pre></td></tr></table></figure>
<ul>
<li><code>image</code> 是输入/输出图像，坐标将会在这张图像上绘制（通常就是检测marker的那张图像）。</li>
<li><code>cameraMatrix</code> 和 <code>distCoeffs</code> 是相机校准参数。</li>
<li><code>rvec</code> 和 <code>tvec</code> 是Pose参数，指明了坐标绘制的位置。</li>
<li>最后一个参数是坐标轴的长度，和tvec单位一样（通常是米）。</li>
</ul>
<p>针对一个marker的pose检测的基本的完整示例：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cv::VideoCapture inputVideo;</span><br><span class="line">inputVideo.open(<span class="number">0</span>);</span><br><span class="line">cv::Mat cameraMatrix, distCoeffs; <span class="comment">// camera parameters are read from somewhere </span></span><br><span class="line"></span><br><span class="line">readCameraParameters(cameraMatrix, distCoeffs);</span><br><span class="line"></span><br><span class="line">cv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (inputVideo.grab()) &#123; </span><br><span class="line">	cv::Mat image, imageCopy;</span><br><span class="line">	inputVideo.retrieve(image); </span><br><span class="line">	image.copyTo(imageCopy);</span><br><span class="line">	<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ids; </span><br><span class="line">	<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;cv::Point2f&gt; &gt; corners; </span><br><span class="line">	cv::aruco::detectMarkers(image, dictionary, corners, ids);<span class="comment">// if at least one marker detected </span></span><br><span class="line">	<span class="keyword">if</span> (ids.size() &gt; <span class="number">0</span>) &#123; </span><br><span class="line">		cv::aruco::drawDetectedMarkers(imageCopy, corners, ids);</span><br><span class="line">        <span class="built_in">vector</span>&lt; Mat &gt; rvecs, tvecs; </span><br><span class="line">        cv::aruco::estimatePoseSingleMarkers(corners, <span class="number">0.05</span>, cameraMatrix, distCoeffs, rvecs, tvecs); <span class="comment">// draw axis for each marker </span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;ids.size(); i++) </span><br><span class="line">        &#123;</span><br><span class="line">            cv::aruco::drawAxis(imageCopy, cameraMatrix, distCoeffs, rvecs[i], tvecs[i], <span class="number">0.1</span>); </span><br><span class="line">        &#125;</span><br><span class="line">        cv::imshow(<span class="string">&quot;out&quot;</span>, imageCopy); </span><br><span class="line">        <span class="keyword">char</span> key = (<span class="keyword">char</span>) cv::waitKey(waitTime); </span><br><span class="line">        <span class="keyword">if</span> (key == <span class="number">27</span>) <span class="keyword">break</span>; </span><br><span class="line"></span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>​    样例视频:</p>
<p>​    <a target="_blank" rel="noopener" href="https://youtu.be/IsXWrcB_Hvs"><img src="http://img.youtube.com/vi/IsXWrcB_Hvs/0.jpg" alt="ArUco markers detection video"></a></p>
<p>​    完整的工程代码包含在模块样例文件夹中的 <code>detect_markers.cpp</code></p>
<h2 id="选择字典"><a href="#选择字典" class="headerlink" title="选择字典"></a>选择字典</h2><p>aruco模块提供了Dictionary类来描述marker的字典。</p>
<p>除了marker大小和字典中的marker数目，字典还有一个很重要的参数，就是内部marker的距离。内部marker的距离是marker之间的最小距离，它决定了字典错误检测和纠正能力。</p>
<p>一般而言，较小的字典大小和较大的marker大小将会产生更大的内部marker距离，反之亦然。但是，过大的Marker在检测中更加困难，因为我们需要从图像中提取出更多位的信息。</p>
<p> 例如，如果你的应用中仅仅需要10个marker，最好使用只包含10个marker的字典，而不是包含1000个marker的字典。原因在于，由10个marker组成的字典将会有更大的内部Marker距离，因此，它的容错性更强。</p>
<p>结果，aruco模块包含了很多选择marker字典的途径，所以你可以让你的系统变得更加健壮。</p>
<ul>
<li>预定义的字典：</li>
</ul>
<p>这是选择字典最简单的办法。aruco模块包含了一系列预定义的字典，涵盖了不同的marker大小和marker数量。例如：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250); </span><br></pre></td></tr></table></figure>
<p>DICT_6X6_250 是一个预定义的字典，它包含6x6位的marker，总共有250个marker。</p>
<p>在所有提供的字典中，我们推荐使用你选择尽可能小的marker。例如，如果你需要6x6位的200个marker，选择DICT_6X6_250要优于选择DICT_6X6_1000。字典越小，内部距离就越大。</p>
<ul>
<li>自动生成的字典:</li>
</ul>
<p>我们可以针对想要的marker数量和位来生成字典，以得到最优的内部Marker距离。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cv::aruco::Dictionary dictionary = cv::aruco::generateCustomDictionary(<span class="number">36</span>, <span class="number">5</span>); </span><br></pre></td></tr></table></figure>
<p>这将会产生一个由36个5X5位字典组成的标准字典。这个过程需要几秒钟，具体时间取决于你的参数（更大的字典和更多的位数会耗费更多的时间）</p>
<ul>
<li>手动生成的字典:</li>
</ul>
<p>最终，我们可以手动设置字典，方便做任何修改。为了做到这一点，我们需要手动给 <code>Dictionary</code> 对象参数赋值。必须注意的是，除非你有一些特别的理由需要自己来生成字典，一般情况下我们推荐前面的任一种方案。</p>
<p>字典参数为：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dictionary</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">	<span class="keyword">public</span>:</span><br><span class="line">    	Mat bytesList; </span><br><span class="line">    	<span class="keyword">int</span> markerSize; </span><br><span class="line">    	<span class="keyword">int</span> maxCorrectionBits; <span class="comment">// maximum number of bits that can be corrected</span></span><br><span class="line">    	...</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>bytesList</code> 是一个数组，它包含了所有marker代码的信息。markerSize是每个marker的维度（例如，参数为5代表5x5位）。最终， <code>maxCorrectionBits</code>是marker检测中 可校正的最大比特数，如果这个值过大，会得到大量的错误位置。</p>
<p> <code>bytesList</code> 中的每一行代表字典中的一个marker。但是，这些marker的数据并不以二进制形式存储，而是以一种特殊的方式存储，这样可以简化检测的过程。</p>
<p>幸运的是，我们可以简单地调用静态方法<code>Dictionary::getByteListFromBits()</code>来转换到这种形式。</p>
<p>示例：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">Dictionary dictionary; </span><br><span class="line"><span class="comment">// markers of 6x6 bits </span></span><br><span class="line">dictionary.markerSize = <span class="number">6</span>; </span><br><span class="line"><span class="comment">// maximum number of bit corrections </span></span><br><span class="line">dictionary.maxCorrectionBits = <span class="number">3</span>;</span><br><span class="line"><span class="comment">// lets create a dictionary of 100 markers </span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">100</span>; i++)</span><br><span class="line">&#123;<span class="comment">// assume generateMarkerBits() generate a new marker in binary format, so that </span></span><br><span class="line"><span class="comment">// markerBits is a 6x6 matrix of CV_8UC1 type, only containing 0s and 1s </span></span><br><span class="line">	cv::Mat markerBits = generateMarkerBits(); </span><br><span class="line">	cv::Mat markerCompressed = getByteListFromBits(markerBits); <span class="comment">// add the marker as a new row</span></span><br><span class="line">	dictionary.bytesList.push_back(markerCompressed); </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h2 id="检测参数"><a href="#检测参数" class="headerlink" title="检测参数"></a>检测参数</h2><p> <a target="_blank" rel="noopener" href="http://docs.opencv.org/3.1.0/d9/d6a/group__aruco.html#ga306791ee1aab1513bc2c2b40d774f370">detectMarkers()</a>的一个参数是DetectorParameters对象。这一对象包含了marker检测过程中所有特定的选项。</p>
<p>在这一章节中，我们将介绍所有的参数。我们可以根据它们涉及的阶段来给这些参数分类。</p>
<h4 id="阈值化"><a href="#阈值化" class="headerlink" title="阈值化"></a>阈值化</h4><p>检测的第一步是输入图像的阈值化。</p>
<p>例如，上述样例中的图像阈值化的结果如下：</p>
<p><img src="http://docs.opencv.org/3.1.0/singlemarkersthresh.png" alt="singlemarkersthresh.png"></p>
<p>这一阈值化过程由以下几个参数决定：</p>
<ul>
<li><code>int adaptiveThreshWinSizeMin</code>, <code>intadaptiveThreshWinSizeMax</code>, <code>int adaptiveThreshWinSizeStep</code></li>
</ul>
<p>​    <code>adaptiveThreshWinSizeMin</code> 和 <code>adaptiveThreshWinSizeMax</code> 参数代表选择的自适应阈值窗口大小（以像素为单位）间隔（具体参见<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/opencv">opencv</a>的 <code>threshold()</code>函数）。</p>
<p>参数adaptiveThreshWinSizeStep表明了窗口从adaptiveThreshWinSizeMin到adaptiveThreshWinSizeMax大小的增量。</p>
<p>例如，对于adaptiveThreshWinSizeMin=5，adaptiveThreshWinSizeMax=21以及adaptiveThreshWinSizeStep=4，那么将会产生5个阈值化步骤，窗口大小分别为5, 9, 13, 17 和 21。在每个阈值化图像中，都会选出一些marker候选。</p>
<p>如果marker大小太大的话，较小的窗口大小可能会切割marker的边界，所以它将不会被检测到，就像下图一样：</p>
<p><img src="http://docs.opencv.org/3.1.0/singlemarkersbrokenthresh.png" alt="singlemarkersbrokenthresh.png"></p>
<p>Broken marker image</p>
<p>另一方面，如果marker太小的话，较大的窗口大小也会有类似的效果。此外这一过程将会趋向于全局阈值，而失去了自适应的特性。</p>
<p>最简单的例子是对adaptiveThreshWinSizeMin和 <code>adaptiveThreshWinSizeMax</code>使用相同的值，这样就只会执行一次阈值化步骤。但是，最好还是使用一个范围的值作为窗口大小，虽然较多的阈值化步骤会在一定程度上降低性能。</p>
<p>缺省参数：<code>adaptiveThreshWinSizeMin</code>: 3, <code>adaptiveThreshWinSizeMax</code>: 23, <code>adaptiveThreshWinSizeStep</code>: 10</p>
<ul>
<li><p><code>double adaptiveThreshConstant</code></p>
<p>这一参数表达了阈值状态下的常量（参见Opencv函数）。它的默认值是大多数例子下较好的情况。</p>
</li>
</ul>
<p>默认值: 7</p>
<h4 id="轮廓滤波"><a href="#轮廓滤波" class="headerlink" title="轮廓滤波"></a>轮廓滤波</h4><p>阈值化之后，我们需要检测轮廓。但是，我们并不会把所有的轮廓都当作是候选。在不同步骤中，我们通过滤波剔除一些不太可能是marker的轮廓。这一章节中的参数可以自定义这一过程。</p>
<p> 需要注意到，大多数例子中我们需要平衡检测的性能和效率。所有考虑到的轮廓都会在接下来的过程中做进一步处理，这通常产生了更高的计算消耗。所以，我们希望能够在这一阶段就丢弃错误的候选，而不是下一阶段继续处理。</p>
<p>另一方面，如果滤波的条件过于苛刻，事实上的marker轮廓可能会被错误地剔除，因此，没有检测到marker。</p>
<ul>
<li><p><code>double minMarkerPerimeterRate</code>, <code>doublemaxMarkerPerimeterRate</code></p>
<p>这些参数决定了marker的最小值和最大值，具体来说，是最大最小marker的周长。它们并不是以绝对像素值作为单位，而是相对于输入图片的最大尺寸指定的。</p>
</li>
</ul>
<p>例如，大小为640x480，最小相对marker周长为0.05的图像，将会产生一个最小周长640x0.05 = 32(像素)的marker，因为640是图像的最大尺寸。参数 <code>maxMarkerPerimeterRate</code> 也是类似的。</p>
<p> 如果 <code>minMarkerPerimeterRate</code>太小，检测阶段性能会降低，因为会有更多的轮廓进入到接下来的阶段。这一弊端对于 <code>maxMarkerPerimeterRate</code>参数而言不是那么显著，因为小的轮廓数目通常要多于大的轮廓。选取 <code>minMarkerPerimeterRate</code>值为0以及值为4，就相当于考虑了图像中的所有轮廓，但是出于性能考虑这是不推荐的。</p>
<p> 缺省值：</p>
<p>Default values:<code>minMarkerPerimeterRate</code> : 0.03, <code>maxMarkerPerimeterRate</code> : 4.0</p>
<ul>
<li><code>double polygonalApproxAccuracyRate</code></li>
</ul>
<p>我们对所有的候选进行多边形近似，只有近似结果为方形的形状才能通过<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/softwaretest">测试</a>。这一值决定了多边形近似产生的最大误差（参见<code>approxPolyDP()</code> 函数）。</p>
<p> 这一参数是相对于候选长度的（像素上）。所以如果候选的周长为100像素，polygonalApproxAccuracyRate的值为0.04，那么最大的误差应当为100x0.04=5.4像素。</p>
<p>在大多例子中，缺省参数的表现已经很好了，但对高失真的图像，我们需要更大的误差值。</p>
<p>​    缺省值：0.05</p>
<ul>
<li><code>double minCornerDistanceRate</code></li>
</ul>
<p>同一张marker中每一对角的最小距离。这是相对于marker周长的值。像素的最小距离为Perimeter * minCornerDistanceRate.</p>
<p>​    缺省值: 0.05</p>
<ul>
<li><p><code>double minMarkerDistanceRate</code></p>
<p>两张不同的marker之间的任一对角的最小距离。它表示相对于两个marker的最小标记周长。如果两个候选太接近，较小的一个被忽略。</p>
</li>
</ul>
<p>​    缺省值：0.05</p>
<ul>
<li><code>int minDistanceToBorder</code></li>
</ul>
<p>marker角到图像边缘最小距离。部分图像边缘被遮挡的marker也能被正确地检测出来，如果遮挡部分比较小的话。但是，如果其中一个角被挡住了，返回的角通常在图像边界的一个错误的位置。</p>
<p>如果marker角的位置很重要的话，例如你想要做pose检测，最好舍弃掉那些离图像边缘太接近的角。否则就没有必要。</p>
<p>​    缺省值：3</p>
<h4 id="比特位提取"><a href="#比特位提取" class="headerlink" title="比特位提取"></a>比特位提取</h4><p>检测到候选之后，我们需要分析每个候选的比特位，来确定它们是不是marker。</p>
<p>在分析二进制编码之前，我们需要提取出比特位。为了达到这个目的，将对透视变换后的图像使用Otsu进行阈值化，来分离黑色和白色像素。</p>
<p>以下是一个透视变换后的图像：</p>
<p><img src="http://docs.opencv.org/3.1.0/removeperspective.png" alt="removeperspective.png"></p>
<p>接下来，图像被划分为网格，和marker位数相同。在每个单元格里，我们统计黑色和白色的个数，决定这个单元格的比特位。</p>
<p><img src="http://docs.opencv.org/3.1.0/bitsextraction1.png" alt="bitsextraction1.png"></p>
<p>以下参数可以自定义这一过程：</p>
<ul>
<li><code>int markerBorderBits</code></li>
</ul>
<p>这一参数指定了marker边界的宽度。这和每个比特位的大小相关。因此，值为2意味着边界的长度是两个内部比特位的长度。</p>
<p>这一参数需要和你使用的Marker边界大小一致，边界的大小可以在绘制函数如 <code>drawMarker()</code>中设置。</p>
<p>​    缺省值：1</p>
<ul>
<li><code>double minOtsuStdDev</code></li>
</ul>
<p>这个值决定了进行Otsu的最小标准差的像素值。如果偏差很低，这可能意味着所有方形都是黑色的(或白色的），Ostu将不起作用。如果是这样的话，所有的比特位都根据平均值大于还是小于128被设为0或者1.</p>
<p>​    缺省值：5.0</p>
<ul>
<li><code>int perpectiveRemovePixelPerCell</code></li>
</ul>
<p>这一参数决定了透视变换后图像的像素数目（每个单元格，包含边界）。这是上图中红色正方形的大小。</p>
<p>例如，让我们假设我们在处理5x5比特位、边界为1比特位的marker（参见markerBorderBit）。然后，每一维的单元格/比特位的个数为：5 + 2* 1 = 7（边界需要被统计2次）。单元格总体大小为：7x7。</p>
<p>如果perpectiveRemovePixelPerCell的值为10，那么获取到的图像大小为10*7 = 70 -&gt; 70x70 </p>
<p>这一参数选择更大的值可以提升比特位的提取过程（在某一程度上），但是它同样也降低了性能。</p>
<p>​    缺省值：4</p>
<ul>
<li><code>double perspectiveRemoveIgnoredMarginPerCell</code></li>
</ul>
<p>当提取每个单元格的比特位时，需要统计黑色和白色的像素个数。一般而言，我们不推荐考虑单元格的所有像素。反之，最好忽略单元格的一些像素。</p>
<p>原因在于，透视变换之后，单元格的颜色不会完全分离，白色的单元格可能会混入一些黑色的单元格（反之亦然）。因此，最好忽略这些像素，以避免错误的像素计数。</p>
<p>例如，以下图像：</p>
<p><img src="http://docs.opencv.org/3.1.0/bitsextraction2.png" alt="bitsextraction2.png"></p>
<p>我们只考虑处在绿色正方形中的像素。我们可以在右边的图像中看到，最终的像素包含了邻域单元格更少的噪声。参数<code>perspectiveRemoveIgnoredMarginPerCell</code> 指明了红色和绿色正方形之间的距离。</p>
<p>这一参数是相对于单元格整体的大小的。例如，如果单元格的大小为40像素，这一参数的值为0.1，那么大小为40*0.1=4像素的边界将被剔除。这意味着每个单元格实际上要分析的像素大小为32x32，而不是40x40。</p>
<p>​     缺省值：0.13</p>
<h4 id="Marker-ID"><a href="#Marker-ID" class="headerlink" title="Marker ID"></a>Marker ID</h4><p>比特位提取之后，接下来的步骤是检查提取的编码是否属于这个marker字典，有必要的话，还需要做错误检测步骤。</p>
<ul>
<li><p><code>double maxErroneousBitsInBorderRate</code></p>
<p>marker边界的比特位应当是黑色的。这一参数指明了允许的边界出错比特位的个数。如，边界可以出现的白色比特位的最大值。它的大小相对于marker中的比特位总数。</p>
</li>
</ul>
<p>​    缺省值：0.35</p>
<ul>
<li><code>double errorCorrectionRate</code></li>
</ul>
<p>每个marker字典有一位可以纠正的理论最大值（Dictionary.maxCorrectionBits）。但是，这个值可以由<code>errorCorrectionRate</code> 参数来修改。</p>
<p> 例如，如果允许纠正的比特位（对于使用的字典）数目为6， <code>errorCorrectionRate</code>的值为0.5，那么实际上最大的可以纠正的比特位个数为6*0.5=3</p>
<p>这一值对减少错误容忍率以避免错误的位置识别很有帮助。</p>
<p>​    缺省值：0.6</p>
<h4 id="角落细化（Corner-Refinement）"><a href="#角落细化（Corner-Refinement）" class="headerlink" title="角落细化（Corner Refinement）"></a>角落细化（Corner Refinement）</h4><p>当我们检测完marker，并且验证了它们的id之后，最后要做的一步是在角落处的亚像素级的细化（参见OpenCV <code>cornerSubPix()</code>)</p>
<p>注意，这一步是可选的，仅在我们对marker角位置的准确性要求很高时才有意义。例如，pose的检测。这一步骤很耗费时间，所以默认下是不做的。</p>
<ul>
<li><code>bool doCornerRefinement</code></li>
</ul>
<p>这一参数决定了是否要进行角落亚像素级细化过程，如果对角点的准确性要求不高，可以不进行这一过程。</p>
<p>​    缺省值：false</p>
<ul>
<li><code>int cornerRefinementWinSize</code></li>
</ul>
<p>这一参数决定了亚像素级细化过程的窗口大小。</p>
<p>较大的值可以产生窗口区域内比较靠近的图像角，marker角会移动到一个不同的错误的地方。除此之外这还会影响到性能。</p>
<p>​    缺省值：5</p>
<ul>
<li><code>int cornerRefinementMaxIterations</code>, <code>doublecornerRefinementMinAccuracy</code></li>
</ul>
<p>这两个值决定了亚像素级细化过程的结束条件。cornerRefinementMaxIterations指明了迭代的最大次数，<code>cornerRefinementMinAccuracy</code> 是结束这一过程前的最小错误值。</p>
<p>如果迭代次数过高，这会影响到性能。此外，如果太小的话，亚像素级细化就基本没有发挥作用。</p>
<p>​    缺省值：</p>
<p><code>cornerRefinementMaxIterations</code>: 30, <code>cornerRefinementMinAccuracy</code>: 0.1</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Wang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://junlu.club/2020/09/17/OpenCV%E4%B8%AD%E7%9A%84Aruco%E8%AF%86%E5%88%AB/">http://junlu.club/2020/09/17/OpenCV中的Aruco识别/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://junlu.club" target="_blank">Go Further</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Aruco/">Aruco</a></div><div class="post_share"><div class="social-share" data-image="/img/av.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/09/17/Augument-Reality-Review-1/"><img class="prev-cover" src="/null" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Augument Reality : Review 1</div></div></a></div><div class="next-post pull-right"><a href="/2020/09/15/%E5%85%B3%E4%BA%8EWindows10%E8%AE%BE%E7%BD%AEWindows-terminal-%E5%BD%93%E5%89%8D%E7%9B%AE%E5%BD%95%E6%89%93%E5%BC%80/"><img class="next-cover" src="/null" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">关于Windows10设置Windows terminal 当前目录打开</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By Wang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  var script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    $.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js', function () {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>function loadValine () {
  function initValine () {
    window.valine = new Valine({
      el: '#vcomment',
      appId: 'KFCqjgf0rMWEOARLXifgdoTP-gzGzoHsz',
      appKey: 'f2AGSKWVYlQWgd6deOHoLq8e',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    });
    if ('nick,mail') { valine.config.requiredFields= 'nick,mail'.split(',') }
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/third-party/ClickShowText.js" async="async"></script></div></body></html>